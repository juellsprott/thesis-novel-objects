{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOUN Dataset + BLIP-2 Multimodal Model Pipeline\n",
    "#### This notebook contains the pipeline for loading the BLIP2 Opt-2.7b model and running inference on the NOUN Dataset\n",
    "\n",
    "Note that for this pipeline it is recommended to use a GPU with sufficient RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports\n",
    "Import modules, requires the installation of bitsandbytes and accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes accelerate Pillow git+https://github.com/huggingface/transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load model\n",
    "uses bitsandbytes to allow int8 quanitization for greatly reduced memory usage, allowing the model to be run on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /gpfs/home5/jsprott/thesis-novel-objects/noun2-env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /sw/arch/RHEL8/EB_production/2022/software/CUDA/11.7.0/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /gpfs/home5/jsprott/thesis-novel-objects/noun2-env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f62c7050874c4eb711b663615c7d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load processor\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# load in float16 # load in int8\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\",\n",
    "                                                      load_in_8bit=True, device_map=\"auto\")\n",
    "# setup device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform inference on NOUN Dataset\n",
    "Currently uses default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m157.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./noun2-env/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in ./noun2-env/lib/python3.10/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: six>=1.5 in ./noun2-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.1 pytz-2023.3 tzdata-2023.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/NOUN-2-600DPI/2001-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2002-600.jpg has generated: red modern abstract sculpture with a spiral pattern\n",
      "data/NOUN-2-600DPI/2003-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2004-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2005-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2006-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2007-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2008.jpg has generated: | james - james - knot toys -\n",
      "data/NOUN-2-600DPI/2009-600.jpg has generated: - cat toys\n",
      "data/NOUN-2-600DPI/2010-600.jpg has generated: a corona - red\n",
      "data/NOUN-2-600DPI/2011-600.jpg has generated: | free image - free image\n",
      "data/NOUN-2-600DPI/2012-600.jpg has generated: an aussie jellyfish\n",
      "data/NOUN-2-600DPI/2013-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2014-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2015-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2016 600dpi.jpg has generated: \n",
      "data/NOUN-2-600DPI/2017-600.jpg has generated: a metal cat toy with a blue and yellow plastic\n",
      "data/NOUN-2-600DPI/2018-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2019-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2020.jpg has generated: \n",
      "data/NOUN-2-600DPI/2021-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2022-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2023-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2024-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2025-600.jpg has generated: blue\n",
      "data/NOUN-2-600DPI/2026-600.jpg has generated: what do you feel about it?\n",
      "data/NOUN-2-600DPI/2027-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2028-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2029-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2030-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2031.jpg has generated: \n",
      "data/NOUN-2-600DPI/2032-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2033-600.jpg has generated: [ orange ]\n",
      "data/NOUN-2-600DPI/2034-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2035-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2036-600.jpg has generated: - wooden baby dummies\n",
      "data/NOUN-2-600DPI/2037-600.jpg has generated: what are you looking for?\n",
      "data/NOUN-2-600DPI/2038-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2039-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2040-600.jpg has generated: | pet shop - pet supplies | tangle free\n",
      "data/NOUN-2-600DPI/2041-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2042-600.jpg has generated: blue blue yellow\n",
      "data/NOUN-2-600DPI/2043.jpg has generated: \n",
      "data/NOUN-2-600DPI/2044-600.jpg has generated: view full size\n",
      "data/NOUN-2-600DPI/2045-600.jpg has generated: image 1\n",
      "data/NOUN-2-600DPI/2046-600.jpg has generated: 3d images\n",
      "data/NOUN-2-600DPI/2047-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2048-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2049-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2050-600.jpg has generated: the black plastic device\n",
      "data/NOUN-2-600DPI/2051-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2052-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2053-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2054-600.jpg has generated: what is the image that you think the image shows\n",
      "data/NOUN-2-600DPI/2055-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2056-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2057-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2058-600.jpg has generated: pink\n",
      "data/NOUN-2-600DPI/2059-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2060-600.jpg has generated: view images of the item\n",
      "data/NOUN-2-600DPI/2061-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2062-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2063-600.jpg has generated: what do you think about the design?\n",
      "data/NOUN-2-600DPI/2064-600.jpg has generated: \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define path to input and output files\n",
    "input_file = 'dataset_nucleus.csv'\n",
    "\n",
    "\n",
    "# Define question for checking textures (unused for now)\n",
    "QUESTION = \"what do you see in the image?\"\n",
    "\n",
    "# Load data from input file into a pandas DataFrame\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# https://github.com/huggingface/transformers/issues/22146\n",
    "# the above link contains more information on param tweaking\n",
    "# beam search: \n",
    "# model.generate(**inputs, num_beams=5, max_new_tokens=30, repetition_penalty=1.0, length_penalty=1.0, temperature=1)\n",
    "# nucleus sampling:\n",
    "# model.generate(**inputs, do_sample=True, top_p=0.9)\n",
    "# TODO: research how beam search and nucleus sampling work and what other params can be changed\n",
    "\n",
    "# Define function to generate text using the model\n",
    "def generate_text(row):\n",
    "    raw_image = Image.open(row[0].replace(\"\\\\\", \"/\")).convert(\"RGB\")\n",
    "    inputs = processor(raw_image, text=QUESTION, return_tensors=\"pt\").to(DEVICE, torch.float16)\n",
    "    generated_ids = model.generate(**inputs, do_sample=True, top_p=0.9, max_new_tokens=10)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    print(f\"{row[0]} has generated: {generated_text}\")\n",
    "    return generated_text\n",
    "\n",
    "# Add new column with generated text using the apply() method and a lambda function\n",
    "data['BLIP-2, OPT-2.7b question, nucleus sampling'] = data.apply(lambda row: generate_text(row), axis=1)\n",
    "\n",
    "# Write updated data to output file\n",
    "# data.to_csv(input_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from IPython.display import HTML\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Load the image\n",
    "    img = Image.open(row['image_path'].replace(\"\\\\\", \"/\"))\n",
    "    img = img.resize((100, 100))\n",
    "    \n",
    "    # Convert the image to a supported format\n",
    "    with BytesIO() as buffer:\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        img_bytes = buffer.getvalue()\n",
    "\n",
    "    # Encode the image as base64\n",
    "    img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "    \n",
    "    # Create a HTML table with the image and text\n",
    "    html = f'<table><tr><td><img src=\"data:image/png;base64,{img_base64}\" /></td><td><p><strong>Label:</strong> {row[\"actual name\"]}</p><p><strong>Prediction:</strong> {row[\"BLIP-2 OPT-2.7b descriptions\"]}</p></td></tr></table>'\n",
    "    \n",
    "    # Display the HTML table\n",
    "    display(HTML(html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from evaluate import check_colors_and_textures\n",
    "\n",
    "# Load dataset into DataFrame\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Add new column name to header row\n",
    "df.rename(columns={df.columns[-1]: 'BLIP-2, OPT-2.7b evaluation: color and texture'}, inplace=True)\n",
    "\n",
    "# Add new column data to remaining rows\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if pd.notnull(row.iloc[-2]):\n",
    "        colors, textures = check_colors_and_textures(row.iloc[-2])\n",
    "        colors = \", \".join(colors) if len(colors) > 0 else None\n",
    "        textures = \", \".join(textures) if len(textures) > 0 else None\n",
    "        df.at[i, 'BLIP-2, OPT-2.7b evaluation: color and texture'] = f\"{colors}; {textures}\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noun2",
   "language": "python",
   "name": "noun2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
