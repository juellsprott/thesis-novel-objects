{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOUN Dataset + BLIP-2 Multimodal Model Pipeline\n",
    "#### This notebook contains the pipeline for loading the BLIP2 Opt-2.7b model and running inference on the NOUN Dataset\n",
    "\n",
    "Note that for this pipeline it is recommended to use a GPU with sufficient RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports\n",
    "Import modules, requires the installation of bitsandbytes and accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes accelerate Pillow git+https://github.com/huggingface/transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load model\n",
    "uses bitsandbytes to allow int8 quanitization for greatly reduced memory usage, allowing the model to be run on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /gpfs/home5/jsprott/thesis-novel-objects/noun2-env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /sw/arch/RHEL8/EB_production/2022/software/CUDA/11.7.0/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /gpfs/home5/jsprott/thesis-novel-objects/noun2-env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f62c7050874c4eb711b663615c7d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load processor\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# load in float16 # load in int8\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\",\n",
    "                                                      load_in_8bit=True, device_map=\"auto\")\n",
    "# setup device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform inference on NOUN Dataset\n",
    "Currently uses default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m157.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./noun2-env/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in ./noun2-env/lib/python3.10/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: six>=1.5 in ./noun2-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.1 pytz-2023.3 tzdata-2023.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/NOUN-2-600DPI/2001-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2002-600.jpg has generated: red modern abstract sculpture with a spiral pattern\n",
      "data/NOUN-2-600DPI/2003-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2004-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2005-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2006-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2007-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2008.jpg has generated: | james - james - knot toys -\n",
      "data/NOUN-2-600DPI/2009-600.jpg has generated: - cat toys\n",
      "data/NOUN-2-600DPI/2010-600.jpg has generated: a corona - red\n",
      "data/NOUN-2-600DPI/2011-600.jpg has generated: | free image - free image\n",
      "data/NOUN-2-600DPI/2012-600.jpg has generated: an aussie jellyfish\n",
      "data/NOUN-2-600DPI/2013-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2014-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2015-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2016 600dpi.jpg has generated: \n",
      "data/NOUN-2-600DPI/2017-600.jpg has generated: a metal cat toy with a blue and yellow plastic\n",
      "data/NOUN-2-600DPI/2018-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2019-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2020.jpg has generated: \n",
      "data/NOUN-2-600DPI/2021-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2022-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2023-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2024-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2025-600.jpg has generated: blue\n",
      "data/NOUN-2-600DPI/2026-600.jpg has generated: what do you feel about it?\n",
      "data/NOUN-2-600DPI/2027-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2028-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2029-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2030-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2031.jpg has generated: \n",
      "data/NOUN-2-600DPI/2032-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2033-600.jpg has generated: [ orange ]\n",
      "data/NOUN-2-600DPI/2034-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2035-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2036-600.jpg has generated: - wooden baby dummies\n",
      "data/NOUN-2-600DPI/2037-600.jpg has generated: what are you looking for?\n",
      "data/NOUN-2-600DPI/2038-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2039-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2040-600.jpg has generated: | pet shop - pet supplies | tangle free\n",
      "data/NOUN-2-600DPI/2041-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2042-600.jpg has generated: blue blue yellow\n",
      "data/NOUN-2-600DPI/2043.jpg has generated: \n",
      "data/NOUN-2-600DPI/2044-600.jpg has generated: view full size\n",
      "data/NOUN-2-600DPI/2045-600.jpg has generated: image 1\n",
      "data/NOUN-2-600DPI/2046-600.jpg has generated: 3d images\n",
      "data/NOUN-2-600DPI/2047-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2048-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2049-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2050-600.jpg has generated: the black plastic device\n",
      "data/NOUN-2-600DPI/2051-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2052-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2053-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2054-600.jpg has generated: what is the image that you think the image shows\n",
      "data/NOUN-2-600DPI/2055-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2056-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2057-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2058-600.jpg has generated: pink\n",
      "data/NOUN-2-600DPI/2059-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2060-600.jpg has generated: view images of the item\n",
      "data/NOUN-2-600DPI/2061-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2062-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2063-600.jpg has generated: what do you think about the design?\n",
      "data/NOUN-2-600DPI/2064-600.jpg has generated: \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define path to input and output files\n",
    "input_file = 'dataset_nucleus.csv'\n",
    "\n",
    "\n",
    "# Define question for checking textures (unused for now)\n",
    "QUESTION = \"what do you see in the image?\"\n",
    "\n",
    "# Load data from input file into a pandas DataFrame\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# https://github.com/huggingface/transformers/issues/22146\n",
    "# the above link contains more information on param tweaking\n",
    "# beam search: \n",
    "# model.generate(**inputs, num_beams=5, max_new_tokens=30, repetition_penalty=1.0, length_penalty=1.0, temperature=1)\n",
    "# nucleus sampling:\n",
    "# model.generate(**inputs, do_sample=True, top_p=0.9)\n",
    "# TODO: research how beam search and nucleus sampling work and what other params can be changed\n",
    "\n",
    "# Define function to generate text using the model\n",
    "def generate_text(row):\n",
    "    raw_image = Image.open(row[0].replace(\"\\\\\", \"/\")).convert(\"RGB\")\n",
    "    inputs = processor(raw_image, text=QUESTION, return_tensors=\"pt\").to(DEVICE, torch.float16)\n",
    "    generated_ids = model.generate(**inputs, do_sample=True, top_p=0.9, max_new_tokens=10)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    print(f\"{row[0]} has generated: {generated_text}\")\n",
    "    return generated_text\n",
    "\n",
    "# Add new column with generated text using the apply() method and a lambda function\n",
    "data['BLIP-2, OPT-2.7b question, nucleus sampling'] = data.apply(lambda row: generate_text(row), axis=1)\n",
    "\n",
    "# Write updated data to output file\n",
    "# data.to_csv(input_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def get_thumbnail(path):\n",
    "    i = Image.open(path)\n",
    "    i.thumbnail((150, 150), Image.LANCZOS)\n",
    "    return i\n",
    "\n",
    "def image_base64(im):\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>number label</th>\n",
       "      <th>actual name</th>\n",
       "      <th>familiarity score</th>\n",
       "      <th>nameability score</th>\n",
       "      <th>color saliency</th>\n",
       "      <th>texture saliency</th>\n",
       "      <th>BLIP-2, OPT-2.7b caption, nucleus sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>2001</td>\n",
       "      <td>bee have trap</td>\n",
       "      <td>19</td>\n",
       "      <td>50</td>\n",
       "      <td>66</td>\n",
       "      <td>14</td>\n",
       "      <td>two orange balls shaped like a flower are sitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>2002</td>\n",
       "      <td>bookend</td>\n",
       "      <td>22</td>\n",
       "      <td>83</td>\n",
       "      <td>59</td>\n",
       "      <td>9</td>\n",
       "      <td>this is a red abstract sculpture with a very cur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>2003</td>\n",
       "      <td>fidget toy</td>\n",
       "      <td>59</td>\n",
       "      <td>74</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>a group of colored block blocks arranged into ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>2004</td>\n",
       "      <td>pencil sharpener</td>\n",
       "      <td>41</td>\n",
       "      <td>70</td>\n",
       "      <td>48</td>\n",
       "      <td>52</td>\n",
       "      <td>a orange and yellow round plastic key ring sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>2005</td>\n",
       "      <td>fish tank stone</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>81</td>\n",
       "      <td>13</td>\n",
       "      <td>a pink and blue rubber unicorn horn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  number label   \n",
       "0  <PIL.JpegImagePlugin.JpegImageFile image mode=...          2001  \\\n",
       "1  <PIL.JpegImagePlugin.JpegImageFile image mode=...          2002   \n",
       "2  <PIL.JpegImagePlugin.JpegImageFile image mode=...          2003   \n",
       "3  <PIL.JpegImagePlugin.JpegImageFile image mode=...          2004   \n",
       "4  <PIL.JpegImagePlugin.JpegImageFile image mode=...          2005   \n",
       "\n",
       "        actual name  familiarity score  nameability score  color saliency   \n",
       "0     bee have trap                 19                 50              66  \\\n",
       "1           bookend                 22                 83              59   \n",
       "2        fidget toy                 59                 74              19   \n",
       "3  pencil sharpener                 41                 70              48   \n",
       "4   fish tank stone                  6                 25              81   \n",
       "\n",
       "   texture saliency         BLIP-2, OPT-2.7b caption, nucleus sampling  \n",
       "0                14  two orange balls shaped like a flower are sitt...  \n",
       "1                 9   this is a red abstract sculpture with a very cur  \n",
       "2                 0  a group of colored block blocks arranged into ...  \n",
       "3                52  a orange and yellow round plastic key ring sit...  \n",
       "4                13                a pink and blue rubber unicorn horn  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv('dataset_nucleus.csv')\n",
    "data_test.rename(columns={'image_path': 'image'}, inplace=True)\n",
    "data_test['image'] = data_test.image.map(lambda f: get_thumbnail(f))\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = data_test.to_html(formatters={'image': image_formatter}, escape=False)\n",
    "\n",
    "with open('nucleus.html', 'w') as file:\n",
    "    file.write(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from evaluate import check_colors_and_textures\n",
    "\n",
    "# Load dataset into DataFrame\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Add new column name to header row\n",
    "df.rename(columns={df.columns[-1]: 'BLIP-2, OPT-2.7b evaluation: color and texture'}, inplace=True)\n",
    "\n",
    "# Add new column data to remaining rows\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if pd.notnull(row.iloc[-2]):\n",
    "        colors, textures = check_colors_and_textures(row.iloc[-2])\n",
    "        colors = \", \".join(colors) if len(colors) > 0 else None\n",
    "        textures = \", \".join(textures) if len(textures) > 0 else None\n",
    "        df.at[i, 'BLIP-2, OPT-2.7b evaluation: color and texture'] = f\"{colors}; {textures}\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
