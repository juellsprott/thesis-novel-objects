{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOUN Dataset + BLIP-2 Multimodal Model Pipeline\n",
    "#### This notebook contains the pipeline for loading the BLIP2 Opt-2.7b model and running inference on the NOUN Dataset\n",
    "\n",
    "Note that for this pipeline it is recommended to use a GPU with sufficient RAM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports\n",
    "Import modules, requires the installation of bitsandbytes and accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load model\n",
    "uses bitsandbytes to allow int8 quanitization for greatly reduced memory usage, allowing the model to be run on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processor\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# load in float16 # load in int8\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\",\n",
    "                                                      load_in_8bit=True, device_map=\"auto\")\n",
    "# setup device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform inference on NOUN Dataset\n",
    "Currently uses default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Define path to input and output files\n",
    "input_file = 'dataset.csv'\n",
    "\n",
    "\n",
    "# Define question for checking textures (unused for now)\n",
    "QUESTION = \"which colors do you see in the image?\"\n",
    "\n",
    "# Load data from input file into a pandas DataFrame\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Define function to generate text using the model\n",
    "def generate_text(row):\n",
    "    raw_image = Image.open(row[0]).convert(\"RGB\")\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to(DEVICE, torch.float16)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    return generated_text\n",
    "\n",
    "# Add new column with generated text using the apply() method and a lambda function\n",
    "data['BLIP-2, OPT-2.7b description'] = data.apply(lambda row: generate_text(row), axis=1)\n",
    "\n",
    "# Write updated data to output file\n",
    "data.to_csv(input_file, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('path/to/your/file.csv')\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Display the image using IPython.display.Image\n",
    "    display(Image(filename=row['image_path']))\n",
    "    # Print the associated data\n",
    "    print('Label:', row['actual_name'])\n",
    "    print('Prediction:', row['BLIP-2, OPT-2.7b descriptions'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import check_colors_and_textures\n",
    "\n",
    "# Read data from input file into memory\n",
    "data = []\n",
    "with open('dataset.csv', 'r') as csvinput:\n",
    "    reader = csv.reader(csvinput, delimiter=',')\n",
    "    # Add new column name to header row\n",
    "    header = next(reader)\n",
    "    header.append('BLIP-2, OPT-2.7b evaluation: color and texture')\n",
    "    data.append(header)\n",
    "    # Add new column data to remaining rows\n",
    "    for row in tqdm(reader):\n",
    "        # Use data from one column to generate data for new column\n",
    "        if len(row) > 0:\n",
    "            colors, textures = check_colors_and_textures(row[-1])\n",
    "            colors = colors if len(colors) > 0 else None\n",
    "            textures = textures if len(textures) > 0 else None\n",
    "            data.append(row + [colors, textures])\n",
    "            \n",
    "\n",
    "# # Write updated data with new column back to input file\n",
    "with open('dataset.csv', 'w') as csvoutput:\n",
    "    writer = csv.writer(csvoutput)\n",
    "    writer.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
