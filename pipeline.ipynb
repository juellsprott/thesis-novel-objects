{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOUN Dataset + BLIP-2 Multimodal Model Pipeline\n",
    "#### This notebook contains the pipeline for loading the BLIP2 Opt-2.7b model and running inference on the NOUN Dataset\n",
    "\n",
    "Note that for this pipeline it is recommended to use a GPU with sufficient RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports\n",
    "Import modules, requires the installation of bitsandbytes and accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes accelerate Pillow git+https://github.com/huggingface/transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load model\n",
    "uses bitsandbytes to allow int8 quanitization for greatly reduced memory usage, allowing the model to be run on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3acc186aab49959baf457d27dc1dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /gpfs/home5/jsprott/thesis-novel-objects/noun2-env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /sw/arch/RHEL8/EB_production/2022/software/CUDA/11.7.0/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /gpfs/home5/jsprott/thesis-novel-objects/noun2-env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e21b5e755d949289a94ffc092c8c0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load processor\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# load in float16 # load in int8\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\",\n",
    "                                                      load_in_8bit=True, device_map=\"auto\")\n",
    "# setup device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform inference on NOUN Dataset\n",
    "Currently uses default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/NOUN-2-600DPI/2001-600.jpg has generated: a giant orange soda can.\n",
      "data/NOUN-2-600DPI/2002-600.jpg has generated: kathie b\n",
      "data/NOUN-2-600DPI/2003-600.jpg has generated: an eraser\n",
      "data/NOUN-2-600DPI/2004-600.jpg has generated: an orange and yellow orange ball\n",
      "data/NOUN-2-600DPI/2005-600.jpg has generated: an airbrush unicorn tail\n",
      "data/NOUN-2-600DPI/2006-600.jpg has generated: the object in this image is a metal and glass dish with a top that looks like the back of\n",
      "data/NOUN-2-600DPI/2007-600.jpg has generated: an anti-aircraft weapon\n",
      "data/NOUN-2-600DPI/2008.jpg has generated: a chair\n",
      "data/NOUN-2-600DPI/2009-600.jpg has generated: an orange spiked dog toy with spiky spikes\n",
      "data/NOUN-2-600DPI/2010-600.jpg has generated: a quill\n",
      "data/NOUN-2-600DPI/2011-600.jpg has generated: the object is a green and orange toy\n",
      "data/NOUN-2-600DPI/2012-600.jpg has generated: A polyps\n",
      "data/NOUN-2-600DPI/2013-600.jpg has generated: The red plastic foot stands up on its own two feet and the yellow one goes up into the orange\n",
      "data/NOUN-2-600DPI/2014-600.jpg has generated: a water hose\n",
      "data/NOUN-2-600DPI/2015-600.jpg has generated: a bucket\n",
      "data/NOUN-2-600DPI/2016 600dpi.jpg has generated: \n",
      "data/NOUN-2-600DPI/2017-600.jpg has generated: the object is called the hook - ball attachment\n",
      "data/NOUN-2-600DPI/2018-600.jpg has generated: the box lid\n",
      "data/NOUN-2-600DPI/2019-600.jpg has generated: an easter egg.\n",
      "data/NOUN-2-600DPI/2020.jpg has generated: a stack of four colorful squares\n",
      "data/NOUN-2-600DPI/2021-600.jpg has generated: it's the end of a bunch of tubes, one green and one pink\n",
      "data/NOUN-2-600DPI/2022-600.jpg has generated: a green light\n",
      "data/NOUN-2-600DPI/2023-600.jpg has generated: a black and white polka dot egg\n",
      "data/NOUN-2-600DPI/2024-600.jpg has generated: a snow globe\n",
      "data/NOUN-2-600DPI/2025-600.jpg has generated: a marabou stick\n",
      "data/NOUN-2-600DPI/2026-600.jpg has generated: a green, red, and black toothbrush\n",
      "data/NOUN-2-600DPI/2027-600.jpg has generated: a ball\n",
      "data/NOUN-2-600DPI/2028-600.jpg has generated: a donut\n",
      "data/NOUN-2-600DPI/2029-600.jpg has generated: a round object\n",
      "data/NOUN-2-600DPI/2030-600.jpg has generated: the thing that holds the thing\n",
      "data/NOUN-2-600DPI/2031.jpg has generated: a metal object with wavy shape\n",
      "data/NOUN-2-600DPI/2032-600.jpg has generated: A banana!\n",
      "data/NOUN-2-600DPI/2033-600.jpg has generated: a dolly!\n",
      "data/NOUN-2-600DPI/2034-600.jpg has generated: a pongomodoro\n",
      "data/NOUN-2-600DPI/2035-600.jpg has generated: the toilet brush\n",
      "data/NOUN-2-600DPI/2036-600.jpg has generated: \n",
      "data/NOUN-2-600DPI/2037-600.jpg has generated: a pink rubber cock ring\n",
      "data/NOUN-2-600DPI/2038-600.jpg has generated: A spiralWhat was the reaction like at the airport when people found out that your mom had a\n",
      "data/NOUN-2-600DPI/2039-600.jpg has generated: a fidget spinner\n",
      "data/NOUN-2-600DPI/2040-600.jpg has generated: a toy ball with blue fuzz\n",
      "data/NOUN-2-600DPI/2041-600.jpg has generated: the cross - piece cross\n",
      "data/NOUN-2-600DPI/2042-600.jpg has generated: blue and yellow plastic bottle, no handle\n",
      "data/NOUN-2-600DPI/2043.jpg has generated: a toy\n",
      "data/NOUN-2-600DPI/2044-600.jpg has generated: orange and blue toothpick holder\n",
      "data/NOUN-2-600DPI/2045-600.jpg has generated: a spoon\n",
      "data/NOUN-2-600DPI/2046-600.jpg has generated: the water bottle holder keychain\n",
      "data/NOUN-2-600DPI/2047-600.jpg has generated: A black glass bottle with a small black stripe on the top\n",
      "data/NOUN-2-600DPI/2048-600.jpg has generated: a ballHaha i would love that. I'm glad it is a thing already, but\n",
      "data/NOUN-2-600DPI/2049-600.jpg has generated: the object is a blue metal pen\n",
      "data/NOUN-2-600DPI/2050-600.jpg has generated: A Black rubber ball for a hand held car radio\n",
      "data/NOUN-2-600DPI/2051-600.jpg has generated: a little toilet seat\n",
      "data/NOUN-2-600DPI/2052-600.jpg has generated: A flower vase\n",
      "data/NOUN-2-600DPI/2053-600.jpg has generated: a hook\n",
      "data/NOUN-2-600DPI/2054-600.jpg has generated: a red wooden stick\n",
      "data/NOUN-2-600DPI/2055-600.jpg has generated: A vase\n",
      "data/NOUN-2-600DPI/2056-600.jpg has generated: A plane\n",
      "data/NOUN-2-600DPI/2057-600.jpg has generated: A red-painted plastic fan\n",
      "data/NOUN-2-600DPI/2058-600.jpg has generated: a cup with a lid\n",
      "data/NOUN-2-600DPI/2059-600.jpg has generated: A spoon, with an attached spoon holder in the shape of an oval\n",
      "data/NOUN-2-600DPI/2060-600.jpg has generated: a glassy-eyed jiggler\n",
      "data/NOUN-2-600DPI/2061-600.jpg has generated: a donut\n",
      "data/NOUN-2-600DPI/2062-600.jpg has generated: The ring is an orange rubber ring with spikes on it\n",
      "data/NOUN-2-600DPI/2063-600.jpg has generated: the sun.\n",
      "data/NOUN-2-600DPI/2064-600.jpg has generated: \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from evaluate import check_colors_and_textures\n",
    "\n",
    "# Define path to input and output files\n",
    "input_file = 'data/datasets/dataset_full.csv'\n",
    "output_file = 'data/datasets/dataset_inference.csv'\n",
    "\n",
    "\n",
    "# Define question for checking textures (unused for now)\n",
    "QUESTION = \"Q: what do you call the object in this image? \\n A:\"\n",
    "\n",
    "# Load data from input file into a pandas DataFrame\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# https://github.com/huggingface/transformers/issues/22146\n",
    "# the above link contains more information on param tweaking\n",
    "# beam search: \n",
    "# model.generate(**inputs, num_beams=5, max_new_tokens=30, repetition_penalty=1.0, length_penalty=1.0, temperature=1)\n",
    "# nucleus sampling:\n",
    "# model.generate(**inputs, do_sample=True, top_p=0.9)\n",
    "# TODO: research how beam search and nucleus sampling work and what other params can be changed\n",
    "\n",
    "# Define function to generate text using the model\n",
    "def generate_text(row, decode='greedy'):\n",
    "    raw_image = Image.open(row[0].replace(\"\\\\\", \"/\")).convert(\"RGB\")\n",
    "    inputs = processor(raw_image, text=QUESTION, return_tensors=\"pt\").to(DEVICE, torch.float16)\n",
    "\n",
    "    if decode == 'greedy':\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "    elif decode == 'nucleus':\n",
    "        generated_ids = model.generate(**inputs, do_sample=True, top_p=0.9, max_new_tokens=20)\n",
    "    elif decode == 'beam':\n",
    "        generated_ids = model.generate(**inputs, num_beams=5, max_new_tokens=20, repetition_penalty=1.0, length_penalty=1.0, temperature=1)\n",
    "\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    match = check_colors_and_textures(generated_text)\n",
    "\n",
    "    print(f\"{row[0]} has generated: {generated_text}\")\n",
    "    return generated_text, match\n",
    "\n",
    "# Add new columns with generated text using the apply() method and a lambda function\n",
    "#data['BLIP-2, OPT-2.7b caption, greedy'], data['BLIP-2, greedy, color and textures'] = zip(*data.progress_apply(lambda row: generate_text(row, decode='greedy'), axis=1))\n",
    "data['BLIP-2, OPT-2.7b caption, nucleus sampling'], data['BLIP-2, nucleus, color and textures'] = zip(*data.apply(lambda row: generate_text(row, decode='nucleus'), axis=1))\n",
    "#data['BLIP-2, OPT-2.7b caption, beam search'], data['BLIP-2, beam, color and textures'] = zip(*data.progress_apply(lambda row: generate_text(row, decode='beam'), axis=1))\n",
    "\n",
    "# Write updated data to output file\n",
    "# data.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def get_thumbnail(path):\n",
    "    i = Image.open(path)\n",
    "    i.thumbnail((150, 150), Image.LANCZOS)\n",
    "    return i\n",
    "\n",
    "def image_base64(im):\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/jsprott.2760907/ipykernel_3856690/257187248.py:13: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  i.thumbnail((150, 150), Image.LANCZOS)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/datasets/dataset_inference.csv')\n",
    "data.rename(columns={'image_path': 'image'}, inplace=True)\n",
    "data['image'] = data.image.map(lambda f: get_thumbnail(f))\n",
    "data['BLIP-2, greedy, color and textures'] = data['BLIP-2, greedy, color and textures'].apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n",
    "data['BLIP-2, nucleus, color and textures'] = data['BLIP-2, nucleus, color and textures'].apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n",
    "data['BLIP-2, beam, color and textures'] = data['BLIP-2, beam, color and textures'].apply(lambda x: re.sub(r'[^\\w]', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = data.to_html(formatters={'image': image_formatter}, escape=False)\n",
    "\n",
    "with open('data/datasets/full_inference.html', 'w') as file:\n",
    "    file.write(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from evaluate import check_colors_and_textures\n",
    "\n",
    "# Load dataset into DataFrame\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Add new column name to header row\n",
    "df.rename(columns={df.columns[-1]: 'BLIP-2, OPT-2.7b evaluation: color and texture'}, inplace=True)\n",
    "\n",
    "# Add new column data to remaining rows\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if pd.notnull(row.iloc[-2]):\n",
    "        colors, textures = check_colors_and_textures(row.iloc[-2])\n",
    "        colors = \", \".join(colors) if len(colors) > 0 else None\n",
    "        textures = \", \".join(textures) if len(textures) > 0 else None\n",
    "        df.at[i, 'BLIP-2, OPT-2.7b evaluation: color and texture'] = f\"{colors}; {textures}\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import importlib\n",
    "importlib.reload(evaluate)\n",
    "from evaluate import colors_to_boolean, textures_to_boolean\n",
    "\n",
    "data['color greedy'] = data['BLIP-2, greedy, color and textures'].apply(lambda x: colors_to_boolean(x))\n",
    "data['color nucleus'] = data['BLIP-2, nucleus, color and textures'].apply(lambda x: colors_to_boolean(x))\n",
    "data['color beam'] = data['BLIP-2, beam, color and textures'].apply(lambda x: colors_to_boolean(x))\n",
    "\n",
    "data['texture greedy'] = data['BLIP-2, greedy, color and textures'].apply(lambda x: textures_to_boolean(x))\n",
    "data['texture nucleus'] = data['BLIP-2, nucleus, color and textures'].apply(lambda x: textures_to_boolean(x))\n",
    "data['texture beam'] = data['BLIP-2, beam, color and textures'].apply(lambda x: textures_to_boolean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Assuming you have a DataFrame called 'data' with columns 'Category' and 'Boolean'\n",
    "\n",
    "# Determine the category bins based on the range of values in the 'Category' column\n",
    "num_bins = 5\n",
    "category_bins = pd.cut(data['color saliency'], bins=num_bins)\n",
    "\n",
    "# Create a new column in the DataFrame to represent the category bins\n",
    "data['Category Bin'] = category_bins\n",
    "\n",
    "# Group the data by the category bins and boolean values and calculate the counts for each group\n",
    "grouped_data = data.groupby(['color saliency', 'color beam']).size().unstack()\n",
    "\n",
    "# Create the grouped bar chart\n",
    "bar_trace_false = go.Bar(\n",
    "    x=[str(bin) for bin in grouped_data.index],\n",
    "    y=grouped_data[False],\n",
    "    name='False'\n",
    ")\n",
    "\n",
    "bar_trace_true = go.Bar(\n",
    "    x=[str(bin) for bin in grouped_data.index],\n",
    "    y=grouped_data[True],\n",
    "    name='True'\n",
    ")\n",
    "\n",
    "# Create the layout for the grouped bar chart\n",
    "layout = go.Layout(\n",
    "    title='Grouped Bar Chart: Color saliency bins, combined with included color boolean',\n",
    "    xaxis=dict(title='Category'),\n",
    "    yaxis=dict(title='Counts'),\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "# Create the figure with the bar traces and layout\n",
    "fig = go.Figure(data=[bar_trace_false, bar_trace_true], layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig_json = fig.to_json()\n",
    "with open('grouped_bar_chart_beam.json', 'w') as file:\n",
    "    json.dump(fig_json, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noun2",
   "language": "python",
   "name": "noun2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
