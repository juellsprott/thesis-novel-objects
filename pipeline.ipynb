{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOUN Dataset + BLIP-2 Multimodal Model Pipeline\n",
    "#### This notebook contains the pipeline for loading the BLIP2 Opt-2.7b model and running inference on the NOUN Dataset\n",
    "\n",
    "Note that for this pipeline it is recommended to use a GPU with sufficient RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports\n",
    "Import modules, requires the installation of bitsandbytes and accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes accelerate Pillow git+https://github.com/huggingface/transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Juell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load model\n",
    "uses bitsandbytes to allow int8 quanitization for greatly reduced memory usage, allowing the model to be run on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 5.2\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary c:\\Users\\Juell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117_nocublaslt.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:141: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/ProgramData/DockerDesktop/version-bin')}\n",
      "  warn(msg)\n",
      "c:\\Users\\Juell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:141: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:10<00:00, 35.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# load processor\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# load in float16 # load in int8\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\",\n",
    "                                                      load_in_8bit=True, device_map=\"auto\")\n",
    "# setup device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform inference on NOUN Dataset\n",
    "Currently uses default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [29:38<00:00, 27.79s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from evaluate import check_colors_and_textures\n",
    "\n",
    "# Define path to input and output files\n",
    "input_file = 'data/datasets/dataset_full.csv'\n",
    "output_file = 'data/datasets/dataset_questions.csv'\n",
    "\n",
    "\n",
    "# Define question for checking textures (unused for now)\n",
    "QUESTION_1 = \"Q: yes or no, do you recognize this object? \\n A:\"\n",
    "QUESTION_2 = \"Q: what do you call the object in this image? \\n A:\"\n",
    "QUESTION_3 = \"Q: What do you really think this is? \\n A:\"\n",
    "\n",
    "# Load data from input file into a pandas DataFrame\n",
    "data = pd.read_csv(output_file)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# https://github.com/huggingface/transformers/issues/22146\n",
    "# the above link contains more information on param tweaking\n",
    "# beam search:\n",
    "# model.generate(**inputs, num_beams=5, max_new_tokens=30, repetition_penalty=1.0, length_penalty=1.0, temperature=1)\n",
    "# nucleus sampling:\n",
    "# model.generate(**inputs, do_sample=True, top_p=0.9)\n",
    "# TODO: research how beam search and nucleus sampling work and what other params can be changed\n",
    "\n",
    "# Define function to generate text using the model\n",
    "\n",
    "\n",
    "def generate_text(row, decode='greedy'):\n",
    "    raw_image = Image.open(row[0].replace(\"\\\\\", \"/\")).convert(\"RGB\")\n",
    "    inputs1 = processor(raw_image, return_tensors=\"pt\").to(\n",
    "        DEVICE, torch.float16)\n",
    "    inputs2 = processor(raw_image, text=QUESTION_1,\n",
    "                        return_tensors=\"pt\").to(DEVICE, torch.float16)\n",
    "    inputs3 = processor(raw_image, text=QUESTION_2,\n",
    "                        return_tensors=\"pt\").to(DEVICE, torch.float16)\n",
    "    inputs4 = processor(raw_image, text=QUESTION_3,\n",
    "                        return_tensors=\"pt\").to(DEVICE, torch.float16)\n",
    "\n",
    "    if decode == 'greedy':\n",
    "        generated_ids1 = model.generate(**inputs1, max_new_tokens=20)\n",
    "        generated_ids2 = model.generate(**inputs2, max_new_tokens=20)\n",
    "        generated_ids3 = model.generate(**inputs3, max_new_tokens=20)\n",
    "        generated_ids4 = model.generate(**inputs4, max_new_tokens=20)\n",
    "    elif decode == 'nucleus':\n",
    "        generated_ids1 = model.generate(\n",
    "            **inputs1, do_sample=True, top_p=0.9, max_new_tokens=20)\n",
    "        generated_ids2 = model.generate(\n",
    "            **inputs2, do_sample=True, top_p=0.9, max_new_tokens=20)\n",
    "        generated_ids3 = model.generate(\n",
    "            **inputs3, do_sample=True, top_p=0.9, max_new_tokens=20)\n",
    "        generated_ids4 = model.generate(\n",
    "            **inputs4, do_sample=True, top_p=0.9, max_new_tokens=20)\n",
    "    elif decode == 'beam':\n",
    "        generated_ids1 = model.generate(\n",
    "            **inputs1, num_beams=5, max_new_tokens=20, repetition_penalty=1.0, length_penalty=1.0, temperature=1)\n",
    "        generated_ids2 = model.generate(\n",
    "            **inputs2, num_beams=5, max_new_tokens=20, repetition_penalty=1.0, length_penalty=1.0, temperature=1)\n",
    "        generated_ids3 = model.generate(\n",
    "            **inputs3, num_beams=5, max_new_tokens=20, repetition_penalty=1.0, length_penalty=1.0, temperature=1)\n",
    "        generated_ids4 = model.generate(\n",
    "            **inputs4, num_beams=5, max_new_tokens=20, repetition_penalty=1.0, length_penalty=1.0, temperature=1)\n",
    "\n",
    "    generated_text1 = processor.batch_decode(\n",
    "        generated_ids1, skip_special_tokens=True)[0].strip()\n",
    "    generated_text2 = processor.batch_decode(\n",
    "        generated_ids2, skip_special_tokens=True)[0].strip()\n",
    "    generated_text3 = processor.batch_decode(\n",
    "        generated_ids3, skip_special_tokens=True)[0].strip()\n",
    "    generated_text4 = processor.batch_decode(\n",
    "        generated_ids4, skip_special_tokens=True)[0].strip()\n",
    "    match = check_colors_and_textures(generated_text1)\n",
    "\n",
    "    #print(f\"{row[0]} has generated: {generated_text1}\")\n",
    "    return generated_text1, generated_text2, generated_text3, generated_text4, match\n",
    "\n",
    "\n",
    "# Add new columns with generated text using the apply() method and a lambda function\n",
    "# data['BLIP-2, greedy, caption'], data['BLIP-2, OPT-2.7b greedy, bool'], data['BLIP-2, OPT-2.7b greedy, name'], data[\n",
    "#     'BLIP-2, greedy, real'], data['BLIP-2, greedy, color and textures'] = zip(*data.progress_apply(lambda row: generate_text(row, decode='greedy'), axis=1))\n",
    "\n",
    "# data['BLIP-2, nucleus, caption'], data['BLIP-2, nucleus, bool'], data['BLIP-2, OPT-2.7b nucleus, name'], data[\n",
    "#     'BLIP-2, nucleus, real'], data['BLIP-2,nucleus, color and textures'] = zip(*data.progress_apply(lambda row: generate_text(row, decode='nucleus'), axis=1))\n",
    "\n",
    "data['BLIP-2, beam, caption'], data['BLIP-2, beam, bool'], data['BLIP-2, OPT-2.7b greedy, name'], data[\n",
    "    'BLIP-2, beam, real'], data['BLIP-2, beam, color and textures'] = zip(*data.progress_apply(lambda row: generate_text(row, decode='beam'), axis=1))\n",
    "\n",
    "# Write updated data to output file\n",
    "data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def get_thumbnail(path):\n",
    "    i = Image.open(path)\n",
    "    i.thumbnail((150, 150), Image.LANCZOS)\n",
    "    return i\n",
    "\n",
    "def image_base64(im):\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/datasets/dataset_inference.csv')\n",
    "data.rename(columns={'image_path': 'image'}, inplace=True)\n",
    "data['image'] = data.image.map(lambda f: get_thumbnail(f))\n",
    "data['BLIP-2, greedy, color and textures'] = data['BLIP-2, greedy, color and textures'].apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n",
    "data['BLIP-2, nucleus, color and textures'] = data['BLIP-2, nucleus, color and textures'].apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n",
    "data['BLIP-2, beam, color and textures'] = data['BLIP-2, beam, color and textures'].apply(lambda x: re.sub(r'[^\\w]', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = data.to_html(formatters={'image': image_formatter}, escape=False)\n",
    "\n",
    "with open('data/datasets/full_inference.html', 'w') as file:\n",
    "    file.write(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from evaluate import check_colors_and_textures\n",
    "\n",
    "# Load dataset into DataFrame\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Add new column name to header row\n",
    "df.rename(columns={df.columns[-1]: 'BLIP-2, OPT-2.7b evaluation: color and texture'}, inplace=True)\n",
    "\n",
    "# Add new column data to remaining rows\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if pd.notnull(row.iloc[-2]):\n",
    "        colors, textures = check_colors_and_textures(row.iloc[-2])\n",
    "        colors = \", \".join(colors) if len(colors) > 0 else None\n",
    "        textures = \", \".join(textures) if len(textures) > 0 else None\n",
    "        df.at[i, 'BLIP-2, OPT-2.7b evaluation: color and texture'] = f\"{colors}; {textures}\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import importlib\n",
    "importlib.reload(evaluate)\n",
    "from evaluate import colors_to_boolean, textures_to_boolean\n",
    "\n",
    "data['color greedy'] = data['BLIP-2, greedy, color and textures'].apply(lambda x: colors_to_boolean(x))\n",
    "data['color nucleus'] = data['BLIP-2, nucleus, color and textures'].apply(lambda x: colors_to_boolean(x))\n",
    "data['color beam'] = data['BLIP-2, beam, color and textures'].apply(lambda x: colors_to_boolean(x))\n",
    "\n",
    "data['texture greedy'] = data['BLIP-2, greedy, color and textures'].apply(lambda x: textures_to_boolean(x))\n",
    "data['texture nucleus'] = data['BLIP-2, nucleus, color and textures'].apply(lambda x: textures_to_boolean(x))\n",
    "data['texture beam'] = data['BLIP-2, beam, color and textures'].apply(lambda x: textures_to_boolean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Assuming you have a DataFrame called 'data' with columns 'color saliency', 'color beam', and 'Model'\n",
    "\n",
    "# Determine the category bins based on the range of values in the 'color saliency' column\n",
    "num_bins = 5\n",
    "category_bins = pd.cut(data['color saliency'], bins=num_bins)\n",
    "\n",
    "# Create a new column in the DataFrame to represent the category bins\n",
    "data['Category Bin'] = category_bins\n",
    "\n",
    "# Group the data by the category bins, boolean values, and model, and calculate the counts for each group\n",
    "grouped_data = data.groupby(['Category Bin', 'color beam']).size().unstack().reset_index()\n",
    "\n",
    "# Create the grouped bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "bar_trace_false = go.Bar(\n",
    "    x=grouped_data['Category Bin'],\n",
    "    y=grouped_data[False],\n",
    "    name=f'False - {model}'\n",
    ")\n",
    "\n",
    "bar_trace_true = go.Bar(\n",
    "    x=grouped_data['Category Bin'],\n",
    "    y=grouped_data[True],\n",
    "    name=f'True - {model}'\n",
    ")\n",
    "\n",
    "fig.add_trace(bar_trace_false)\n",
    "fig.add_trace(bar_trace_true)\n",
    "\n",
    "# Create the layout for the grouped bar chart\n",
    "fig.update_layout(\n",
    "    title='Grouped Bar Chart: Color Saliency Bins with Boolean Values',\n",
    "    xaxis=dict(title='Category'),\n",
    "    yaxis=dict(title='Counts'),\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Save the plot as JSON\n",
    "fig_json = fig.to_json()\n",
    "with open('grouped_bar_chart.json', 'w') as file:\n",
    "    file.write(fig_json)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
